@article{studies:state_of_ai_2019,
abstract = {We believe that AI will be a force multiplier on technological progress in our increasingly digital, data-driven world. This is because everything around us today, ranging from culture to consumer products, is a product of intelligence. In this report, we set out to capture a snapshot of the exponential progress in AI with a focus on developments in the past 12 months. Consider this report as a compilation of the most interesting things we've seen that seeks to trigger an informed conversation about the state of AI and its implication for the future. This edition builds on the inaugural State of AI Report 2018, which can be found here. We consider the following key dimensions in our report: Research: Technology breakthroughs and their capabilities. Talent: Supply, demand and concentration of talent working in the field. Industry: Large platforms, financings and areas of application for AI-driven innovation today and tomorrow. China: With two distinct internets, we review AI in China as its own category. Politics: Public opinion of AI, economic implications and the emerging geopolitics of AI.},
author = {Benaich, Nathan and Hogarth, Ian},
file = {:https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430:pdf},
mendeley-groups = {Master},
pages = {126},
title = {{State of AI 2019}},
url = {\url{https://www.stateof.ai/}},
year = {2019}
}

@article{report:Kelnar2019,
abstract = {A list of the human tasks artificial intelligence has mastered.},
author = {Kelnar, David},
booktitle = {MMC Ventures},
file = {:Users/romain.claret/Downloads/The-State-of-AI-2019-Divergence.pdf:pdf},
mendeley-groups = {Master},
pages = {151},
title = {{The State of AI}},
url = {https://www.stateofai2019.com/summary/},
year = {2019}
}


@article{papers:gpt2,
abstract = {Natural language processing tasks, such as ques- tion answering, machine translation, reading com- prehension, and summarization, are typically approached with supervised learning on task- specific datasets. We demonstrate that language models begin to learn these tasks without any ex- plicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the an- swers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and in- creasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested lan- guage modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain co- herent paragraphs of text. These findings suggest a promising path towards building language pro- cessing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/romain.claret/Desktop/TM/Papers/Language Models are Unsupervised Multitask Learners.pdf:pdf},
mendeley-groups = {Master},
title = {{Language Models are Unsupervised Multitask Learners}},
year = {2018}
}

@article{paper:tanon2016,
abstract = {Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two ma-jor collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Pri-mary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.},
author = {Tanon, Thomas Pellissier and Vrande{\v{c}}{\'{i}}c, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia},
doi = {10.1145/2872427.2874809},
file = {:Users/romain.claret/Downloads/44818.pdf:pdf},
isbn = {9781450341431},
journal = {25th International World Wide Web Conference, WWW 2016},
keywords = {Crowdsourcing Systems,Freebase,Semantic Web,Wikidata},
mendeley-groups = {Master},
pages = {1419--1428},
title = {{From freebase to wikidata: The great migration}},
year = {2016}
}

@article{paper:bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
doi = {10.1145/1376616.1376746},
file = {:Users/romain.claret/Downloads/Freebase$\backslash$: A Collaboratively Created Graph Database For Structuring Human Knowledge.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Design,Human factors,Languages},
mendeley-groups = {Master},
pages = {1247--1249},
title = {{Freebase: A collaboratively created graph database for structuring human knowledge}},
year = {2008}
}

@inproceedings{paper:gittens-etal-2017-skip,
    title = "Skip-Gram âˆ’ {Z}ipf + Uniform = Vector Additivity",
    author = "Gittens, Alex  and
      Achlioptas, Dimitris  and
      Mahoney, Michael W.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P17-1007",
    pages = "69--76",
    abstract = "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected {``}side-effect{''} of such models is that their vectors often exhibit compositionality, i.e., \textit{adding} two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., {``}man{''} + {``}royal{''} = {``}king{''}. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",
}

@article{paper:Yin2017,
abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
archivePrefix = {arXiv},
arxivId = {1702.01923},
author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tze, Hinrich},
eprint = {1702.01923},
file = {:Users/romain.claret/Desktop/TM/Papers/Comparative Study of CNN and RNN for Natural Language Processing.pdf:pdf},
mendeley-groups = {Master},
title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
url = {http://arxiv.org/abs/1702.01923},
year = {2017}
}

@article{paper:Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:Users/romain.claret/Desktop/TM/Papers/NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
mendeley-groups = {Master},
pages = {1--15},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}

@article{paper:Tai2015,
abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.00075v3},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
eprint = {arXiv:1503.00075v3},
file = {:Users/romain.claret/Desktop/TM/Papers/Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {1556--1566},
title = {{Improved semantic representations from tree-structured long short-Term memory networks}},
volume = {1},
year = {2015}
}



@article{paper:Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v5},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v5},
file = {:Users/romain.claret/Desktop/TM/Papers/Attention Is All You Need.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Master},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-December},
year = {2017}
}


@article{paper:Lucy2017,
abstract = {Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.11168v1},
author = {Lucy, Li and Gauthier, Jon},
doi = {10.18653/v1/w17-2810},
eprint = {arXiv:1705.11168v1},
file = {:Users/romain.claret/Downloads/Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning.pdf:pdf},
mendeley-groups = {Master},
pages = {76--85},
title = {{Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning}},
year = {2017}
}


@article{paper:Dong2015,
abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
author = {Dong, Li and Wei, Furu and Zhou, Ming and Xu, Ke},
file = {:Users/romain.claret/Desktop/TM/Papers/Question Answering over Freebase with Multi-Column Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {260--269},
title = {{Question answering over freebase with multi-column convolutional neural networks}},
volume = {1},
year = {2015}
}


@article{paper:Severyn2016,
abstract = {In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.},
archivePrefix = {arXiv},
arxivId = {1604.01178},
author = {Severyn, Aliaksei and Moschitti, Alessandro},
eprint = {1604.01178},
file = {:Users/romain.claret/Desktop/TM/Papers/Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {Master},
title = {{Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.01178},
year = {2016}
}


@article{paper:Chen2015,
abstract = {{\textcopyright} 2015 Association for Computational Linguistics. Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-The-Art methods.},
author = {Chen, Yubo and Xu, Liheng and Liu, Kang and Zeng, Daojian and Zhao, Jun},
file = {:Users/romain.claret/Desktop/TM/Papers/Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {167--176},
title = {{Event extraction via dynamic multi-pooling convolutional neural networks}},
volume = {1},
year = {2015}
}


@article{paper:turing,
  added-at = {2012-06-19T15:53:23.000+0200},
  author = {Turing, A. M.},
  biburl = {https://www.bibsonomy.org/bibtex/2c6b8db241dec2cec3477ce771abebb8f/jaeschke},
  copyright = {Copyright © 1950 Oxford University Press},
  interhash = {3f7a151a4f79fe75b4bb148b41279a9b},
  intrahash = {c6b8db241dec2cec3477ce771abebb8f},
  issn = {00264423},
  journal = {Mind},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Oct., 1950},
  keywords = {},
  language = {English},
  number = 236,
  pages = {433--460},
  publisher = {Oxford University Press on behalf of the Mind Association},
  series = {New Series},
  timestamp = {2012-06-19T15:53:23.000+0200},
  title = {Computing Machinery and Intelligence},
  url = {http://www.jstor.org/stable/2251299},
  volume = 59,
  year = 1950
}

@inproceedings{paper:devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{paper:Karras2019stylegan2,
  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  journal = {CoRR},
  volume  = {abs/1912.04958},
  year    = {2019},
}

@inproceedings{paper:rajpurkar-etal-2018-know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@ARTICLE{paper:word2vec,
       author = {{Mikolov}, Tomas and {Chen}, Kai and {Corrado}, Greg and {Dean}, Jeffrey},
        title = "{Efficient Estimation of Word Representations in Vector Space}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2013",
        month = "Jan",
          eid = {arXiv:1301.3781},
        pages = {arXiv:1301.3781},
archivePrefix = {arXiv},
       eprint = {1301.3781},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1301.3781M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{paper:glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = "GloVe: Global Vectors for Word Representation",
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}





@misc{blog:intro_knowledge_graph,
author = {Singhal, Amit},
title = {Official Google Blog: Introducing the Knowledge Graph: things, not strings},
howpublished = {\url{https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html}},
month = {May},
year = {2012},
note = {(Accessed on 10/09/2019)}
}

@misc{website:wikidata,
author = {Foundation, Wikimedia},
title = {Wikidata},
howpublished = {\url{https://www.wikidata.org/wiki/Wikidata:Main_Page}},
month = {October},
year = {2012},
note = {(Accessed on 10/09/2019)}
}

@misc{online:gartner_2019_ai_survey,
author = {Rowsell-Jones, Andy and Howard, Chris},
title = {2019 CIO Survey: CIOs Have Awoken to the Importance of AI},
howpublished = {\url{https://www.gartner.com/en/documents/3897266/2019-cio-survey-cios-have-awoken-to-the-importance-of-ai}},
month = {January},
year = {2019},
note = {(Accessed on 10/09/2019)}
}

@misc{online:futurism_history_infography,
author = {Futurism, LLC},
title = {The History of Chatbots Infographic},
howpublished = {\url{https://futurism.com/images/the-history-of-chatbots-infographic}},
month = {January},
year = {2016},
note = {(Accessed on 10/09/2019)}
}

@misc{website:eliza,
author = {Michal Wallace \& George Dunlop},
title = {Eliza, the Rogerian Therapist},
howpublished = {\url{http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm}},
month = {January},
year = {1999},
note = {(Accessed on 10/09/2019)}
}

@misc{website:person_does_not_exist,
author = {Phil Wang},
title = {This Person Does Not Exist},
howpublished = {\url{https://www.thispersondoesnotexist.com}},
year = {2019},
note = {(Accessed on 10/09/2019)}
}

@article{paper:journals/corr/HerbelotB17,
  author    = {Aur{\'{e}}lie Herbelot and
               Marco Baroni},
  title     = {High-risk learning: acquiring new word vectors from tiny data},
  journal   = {CoRR},
  volume    = {abs/1707.06556},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06556},
  archivePrefix = {arXiv},
  eprint    = {1707.06556},
  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HerbelotB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/PinterGE17,
  author    = {Yuval Pinter and
               Robert Guthrie and
               Jacob Eisenstein},
  title     = {Mimicking Word Embeddings using Subword RNNs},
  journal   = {CoRR},
  volume    = {abs/1707.06961},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06961},
  archivePrefix = {arXiv},
  eprint    = {1707.06961},
  timestamp = {Mon, 13 Aug 2018 16:46:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PinterGE17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{paper:ma-etal-2016-label,
    title = "Label Embedding for Zero-shot Fine-grained Named Entity Typing",
    author = "Ma, Yukun  and
      Cambria, Erik  and
      Gao, Sa",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1017",
    pages = "171--180",
    abstract = "Named entity typing is the task of detecting the types of a named entity in context. For instance, given {``}Eric is giving a presentation{''}, our goal is to infer that {`}Eric{'} is a speaker or a presenter and a person. Existing approaches to named entity typing cannot work with a growing type set and fails to recognize entity mentions of unseen types. In this paper, we present a label embedding method that incorporates prototypical and hierarchical information to learn pre-trained label embeddings. In addition, we adapt a zero-shot learning framework that can predict both seen and previously unseen entity types. We perform evaluation on three benchmark datasets with two settings: 1) few-shots recognition where all types are covered by the training set; and 2) zero-shot recognition where fine-grained types are assumed absent from training set. Results show that prior knowledge encoded using our label embedding methods can significantly boost the performance of classification for both cases.",
}

@article{paper:journals/corr/KimJSR15,
  author    = {Yoon Kim and
               Yacine Jernite and
               David A. Sontag and
               Alexander M. Rush},
  title     = {Character-Aware Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/1508.06615},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.06615},
  archivePrefix = {arXiv},
  eprint    = {1508.06615},
  timestamp = {Fri, 15 Nov 2019 17:16:01 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KimJSR15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  abstract	= {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.}
}

@inproceedings{paper:conf/icml/SantosZ14,
  added-at = {2018-11-26T22:08:31.000+0100},
  author = {dos Santos, Cícero Nogueira and Zadrozny, Bianca},
  biburl = {https://www.bibsonomy.org/bibtex/2b2b3fb1e9cc6589cff42c01705fe6dbe/florianpircher},
  booktitle = {ICML},
  crossref = {conf/icml/2014},
  ee = {http://jmlr.org/proceedings/papers/v32/santos14.html},
  interhash = {07d9dda35154defe5091ac197bc7ed1f},
  intrahash = {b2b3fb1e9cc6589cff42c01705fe6dbe},
  keywords = {final pos_tagging thema:sequence_labeling},
  pages = {1818-1826},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2018-11-27T09:38:32.000+0100},
  title = {Learning Character-level Representations for Part-of-Speech Tagging.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2014.html#SantosZ14},
  volume = 32,
  year = 2014
}

@inproceedings{paper:conf/ijcai/ChenXLSL15,
  author    = {Xinxiong Chen and
               Lei Xu and
               Zhiyuan Liu and
               Maosong Sun and
               Huan{-}Bo Luan},
  title     = {Joint Learning of Character and Word Embeddings},
  booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2015, Buenos Aires, Argentina, July
               25-31, 2015},
  pages     = {1236--1242},
  year      = {2015},
  crossref  = {DBLP:conf/ijcai/2015},
  url       = {http://ijcai.org/Abstract/15/178},
  timestamp = {Tue, 20 Aug 2019 16:18:00 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ijcai/ChenXLSL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{paper:2017HaoYetal,
author="Hao, Yazhou
and Zheng, Qinghua
and Lan, Yangyang
and Li, Yufei
and Wang, Meng
and Wang, Sen
and Li, Chen",
editor="Cong, Gao
and Peng, Wen-Chih
and Zhang, Wei Emma
and Li, Chengliang
and Sun, Aixin",
title="Improving Chinese Sentiment Analysis via Segmentation-Based Representation Using Parallel CNN",
booktitle="Advanced Data Mining and Applications",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="668--680",
abstract="Automatically analyzing sentimental implications in texts relies on well-designed models utilizing linguistic features. Therefore, the models are mostly language-dependent and designed for English texts. Chinese is with the largest users in the world and has a tremendous amount of texts daily generated from the social media, etc. However, it has seldom been studied. On another hand, a general observation, which is valid in many languages, is that different segments of a piece of text, e.g. a clause, having different sentimental polarities. The existing deep learning models neglect the imbalanced sentiment distribution and only take the entire piece of the text. This paper proposes a novel sentiment-analysis model, which is capable of sentiment analysis task in Chinese. Firstly, the model segments a text into smaller units according to the punctuations to obtain the preliminary text representation, and this step is so-called segmentation-based representation. Meanwhile, its new framework parallel-CNN (convolutional neural network) simultaneously use all segments. This model, we call SBR-PCNN, concatenate the representation of each segment to obtain the final representation of the text which does not only contain the semantic and syntactic features but also retains the essential sequential information. The proposed method has been evaluated on two Chinese sentiment classification datasets and compared with a broad range of baselines. Experimental results show that the proposed approach achieves the state of the art results on two benchmarking datasets. Meanwhile, they demonstrate that our model may improve the performance of Chinese sentiment analysis.",
isbn="978-3-319-69179-4"
}

@article{paper:journals/corr/BojanowskiGJM16,
  author    = {Piotr Bojanowski and
               Edouard Grave and
               Armand Joulin and
               Tomas Mikolov},
  title     = {Enriching Word Vectors with Subword Information},
  journal   = {CoRR},
  volume    = {abs/1607.04606},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.04606},
  archivePrefix = {arXiv},
  eprint    = {1607.04606},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BojanowskiGJM16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/LucyG17,
  author    = {Li Lucy and
               Jon Gauthier},
  title     = {Are distributional representations ready for the real world? Evaluating
               word vectors for grounded perceptual meaning},
  journal   = {CoRR},
  volume    = {abs/1705.11168},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.11168},
  archivePrefix = {arXiv},
  eprint    = {1705.11168},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LucyG17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/abs-1802-05365,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Mohit Iyyer and
               Matt Gardner and
               Christopher Clark and
               Kenton Lee and
               Luke Zettlemoyer},
  title     = {Deep contextualized word representations},
  journal   = {CoRR},
  volume    = {abs/1802.05365},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05365},
  archivePrefix = {arXiv},
  eprint    = {1802.05365},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05365},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{paper:NIPS2015_5949,
title = {Semi-supervised Sequence Learning},
author = {Dai, Andrew M and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3079--3087},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf}
}

@article{paper:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:bahdanau2014neural,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  added-at = {2016-09-28T18:44:25.000+0200},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/20d6b4e471c01b5a1d0be959d2f74494d/dallmann},
  interhash = {bb2ca011eeafccb0bd2505c9476dcd10},
  intrahash = {0d6b4e471c01b5a1d0be959d2f74494d},
  journal = {arXiv preprint arXiv:1409.0473},
  keywords = {neural_networks thema thema:attention},
  timestamp = {2016-09-30T12:33:25.000+0200},
  title = {Neural machine translation by jointly learning to align and translate},
  year = 2014
}

@article{paper:journals/corr/abs-1906-05909,
  author    = {Prajit Ramachandran and
               Niki Parmar and
               Ashish Vaswani and
               Irwan Bello and
               Anselm Levskaya and
               Jonathon Shlens},
  title     = {Stand-Alone Self-Attention in Vision Models},
  journal   = {CoRR},
  volume    = {abs/1906.05909},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05909},
  archivePrefix = {arXiv},
  eprint    = {1906.05909},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05909},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:Weston2015MemoryN,
  title={Memory Networks},
  author={Jason Weston and Sumit Chopra and Antoine Bordes},
  journal={CoRR},
  year={2015},
  volume={abs/1410.3916}
}

@article{paper:journals/corr/TangQL16,
  author    = {Duyu Tang and
               Bing Qin and
               Ting Liu},
  title     = {Aspect Level Sentiment Classification with Deep Memory Network},
  journal   = {CoRR},
  volume    = {abs/1605.08900},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.08900},
  archivePrefix = {arXiv},
  eprint    = {1605.08900},
  timestamp = {Mon, 13 Aug 2018 16:46:17 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TangQL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/MillerFDKBW16,
  author    = {Alexander H. Miller and
               Adam Fisch and
               Jesse Dodge and
               Amir{-}Hossein Karimi and
               Antoine Bordes and
               Jason Weston},
  title     = {Key-Value Memory Networks for Directly Reading Documents},
  journal   = {CoRR},
  volume    = {abs/1606.03126},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03126},
  archivePrefix = {arXiv},
  eprint    = {1606.03126},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MillerFDKBW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/KumarISBEPOGS15,
  author    = {Ankit Kumar and
               Ozan Irsoy and
               Jonathan Su and
               James Bradbury and
               Robert English and
               Brian Pierce and
               Peter Ondruska and
               Ishaan Gulrajani and
               Richard Socher},
  title     = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1506.07285},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.07285},
  archivePrefix = {arXiv},
  eprint    = {1506.07285},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KumarISBEPOGS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{paper:convex,
       author = {{Christmann}, Philipp and {Saha Roy}, Rishiraj and
         {Abujabal}, Abdalghani and {Singh}, Jyotsna and {Weikum}, Gerhard},
        title = "{Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Information Retrieval, Computer Science - Computation and Language},
         year = "2019",
        month = "Oct",
          eid = {arXiv:1910.03262},
        pages = {arXiv:1910.03262},
archivePrefix = {arXiv},
       eprint = {1910.03262},
 primaryClass = {cs.IR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191003262C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{paper:journals/corr/BordesUCW15,
  author    = {Antoine Bordes and
               Nicolas Usunier and
               Sumit Chopra and
               Jason Weston},
  title     = {Large-scale Simple Question Answering with Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02075},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02075},
  archivePrefix = {arXiv},
  eprint    = {1506.02075},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BordesUCW15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paper:wikidata-benchmark,
  author    = {Dennis Diefenbach and
               Thomas Pellissier Tanon and
               Kamal Deep Singh and
               Pierre Maret},
  title     = {Question Answering Benchmarks for Wikidata},
  booktitle = {Proceedings of the {ISWC} 2017 Posters {\&} Demonstrations and
               Industry Tracks co-located with 16th International Semantic Web Conference
               {(ISWC} 2017), Vienna, Austria, October 23rd - to - 25th, 2017.},
  year      = {2017},
  url       = {http://ceur-ws.org/Vol-1963/paper555.pdf}
}

@article{paper:journals/corr/abs-1808-07036,
  author    = {Eunsol Choi and
               He He and
               Mohit Iyyer and
               Mark Yatskar and
               Wen{-}tau Yih and
               Yejin Choi and
               Percy Liang and
               Luke Zettlemoyer},
  title     = {QuAC : Question Answering in Context},
  journal   = {CoRR},
  volume    = {abs/1808.07036},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07036},
  archivePrefix = {arXiv},
  eprint    = {1808.07036},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-07036},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/abs-1806-03822,
  author    = {Pranav Rajpurkar and
               Robin Jia and
               Percy Liang},
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  journal   = {CoRR},
  volume    = {abs/1806.03822},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.03822},
  archivePrefix = {arXiv},
  eprint    = {1806.03822},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-03822},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/RajpurkarZLL16,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal   = {CoRR},
  volume    = {abs/1606.05250},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05250},
  archivePrefix = {arXiv},
  eprint    = {1606.05250},
  timestamp = {Mon, 13 Aug 2018 16:49:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RajpurkarZLL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/abs-1808-07042,
  author    = {Siva Reddy and
               Danqi Chen and
               Christopher D. Manning},
  title     = {CoQA: {A} Conversational Question Answering Challenge},
  journal   = {CoRR},
  volume    = {abs/1808.07042},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07042},
  archivePrefix = {arXiv},
  eprint    = {1808.07042},
  timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-07042},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/abs-1809-10735,
  author    = {Mark Yatskar},
  title     = {A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC},
  journal   = {CoRR},
  volume    = {abs/1809.10735},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.10735},
  archivePrefix = {arXiv},
  eprint    = {1809.10735},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-10735},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:google-natural-questions,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}

@article{paper:journals/corr/LowePSP15,
  author    = {Ryan Lowe and
               Nissan Pow and
               Iulian Serban and
               Joelle Pineau},
  title     = {The Ubuntu Dialogue Corpus: {A} Large Dataset for Research in Unstructured
               Multi-Turn Dialogue Systems},
  journal   = {CoRR},
  volume    = {abs/1506.08909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.08909},
  archivePrefix = {arXiv},
  eprint    = {1506.08909},
  timestamp = {Mon, 13 Aug 2018 16:48:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LowePSP15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paper:CIKM-2010-FerraginaS,
	author        = "Paolo Ferragina and Ugo Scaiella",
	booktitle     = "{Proceedings of the 19th ACM International Conference on Conference on Information and Knowledge Management}",
	doi           = "10.1145/1871437.1871689",
	ee            = "http://doi.acm.org/10.1145/1871437.1871689",
	pages         = "1625--1628",
	publisher     = "{ACM}",
	title         = "{TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)}",
	year          = 2010,
}

@article{paper:Lopezetal2013,
  author    = {Lopez, Vanessa and Unger, Christina and Cimiano, Philipp and Motta, Enrico},
  title     = {Evaluating question answering over linked data},
  journal   = {Web Semantics Science Services And Agents On The World Wide Web},
  volume    = {21},
  pages     = {3--13},
  publisher = {Elsevier},
  year      = {2013},
  doi       = {10.1016/j.websem.2013.05.006},
  issn      = {1570-8268}
}

@ARTICLE{paper:2019arXiv190910772J,
       author = {{Ju}, Ying and {Zhao}, Fubang and {Chen}, Shijie and {Zheng}, Bowen and
         {Yang}, Xuefeng and {Liu}, Yunfeng},
        title = "{Technical report on Conversational Question Answering}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Sep",
          eid = {arXiv:1909.10772},
        pages = {arXiv:1909.10772},
archivePrefix = {arXiv},
       eprint = {1909.10772},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190910772J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{paper:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy
                and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-11692},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{paper:InProceedingsPellissier-Tanon.P-TD-d-ACM-S_18,
address = {Heraklion, Greece.},
author = {Pellissier Tanon, Thomas and Dias de Assun{\c c}{\~a}o, Marcos and Caron, Eddy and M. Suchanek, Fabian},
booktitle = {ESWC 2018 - Extended Semantic Web Conference},
month = {June},
note = {hal-01824972},
title = {Demoing {P}latypus -- was {A} {M}ultilingual {Q}uestion {A}nswering {P}latform for {W}ikidata},
year = {2018} 
}

@article{paper:journals/corr/abs-1906-04043,
  author    = {Sebastian Gehrmann and
               Hendrik Strobelt and
               Alexander M. Rush},
  title     = {{GLTR:} Statistical Detection and Visualization of Generated Text},
  journal   = {CoRR},
  volume    = {abs/1906.04043},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04043},
  archivePrefix = {arXiv},
  eprint    = {1906.04043},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-04043},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:10.1177/1464884916641269,
author = {Andreas Graefe and Mario Haim and Bastian Haarmann and Hans-Bernd Brosius},
title ={Readers’ perception of computer-generated news: Credibility, expertise, and readability},
journal = {Journalism},
volume = {19},
number = {5},
pages = {595-610},
year = {2018},
doi = {10.1177/1464884916641269},
URL = {https://doi.org/10.1177/1464884916641269},
eprint = {https://doi.org/10.1177/1464884916641269},
abstract = { We conducted an online experiment to study people’s perception of automated computer-written news. Using a 2 × 2 × 2 design, we varied the article topic (sports, finance; within-subjects) and both the articles’ actual and declared source (human-written, computer-written; between-subjects). Nine hundred eighty-six subjects rated two articles on credibility, readability, and journalistic expertise. Varying the declared source had small but consistent effects: subjects rated articles declared as human written always more favorably, regardless of the actual source. Varying the actual source had larger effects: subjects rated computer-written articles as more credible and higher in journalistic expertise but less readable. Across topics, subjects’ perceptions did not differ. The results provide conservative estimates for the favorability of computer-written news, which will further increase over time and endorse prior calls for establishing ethics of computer-written news. }
}

@article{paper:journals/corr/SeoKFH16,
  author    = {Min Joon Seo and
               Aniruddha Kembhavi and
               Ali Farhadi and
               Hannaneh Hajishirzi},
  title     = {Bidirectional Attention Flow for Machine Comprehension},
  journal   = {CoRR},
  volume    = {abs/1611.01603},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01603},
  archivePrefix = {arXiv},
  eprint    = {1611.01603},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeoKFH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paper:journals/corr/abs-1710-10723,
  author    = {Christopher Clark and
               Matt Gardner},
  title     = {Simple and Effective Multi-Paragraph Reading Comprehension},
  journal   = {CoRR},
  volume    = {abs/1710.10723},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10723},
  archivePrefix = {arXiv},
  eprint    = {1710.10723},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-10723},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{website:awesomenlp-github,
  author = {Keon},
  title = {keon / awesome-nlp},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/keon/awesome-nlp}},
  commit = {4cb17e02763fb216098f24f6f378871f0fa69bd4}
}

@misc{website:akinator,
author = {Elokence, Scimob},
title = {Akinator},
howpublished = {\url{https://en.wikipedia.org/wiki/Akinator}},
year = {2007},
note = {(Accessed on 10/09/2019)}
}

@misc{website:hellojam,
author = {Blackbird SAS},
title = {HelloJam.fr},
howpublished = {\url{https://www.hellojam.fr}},
year = {2014},
note = {(Accessed on 10/09/2019)}
}

@article{paper:journals/corr/TrischlerWYHSBS16,
  author    = {Adam Trischler and
               Tong Wang and
               Xingdi Yuan and
               Justin Harris and
               Alessandro Sordoni and
               Philip Bachman and
               Kaheer Suleman},
  title     = {NewsQA: {A} Machine Comprehension Dataset},
  journal   = {CoRR},
  volume    = {abs/1611.09830},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09830},
  archivePrefix = {arXiv},
  eprint    = {1611.09830},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TrischlerWYHSBS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{paper:2019arXiv191102168W,
       author = {{Wang}, Quan and {Huang}, Pingping and {Wang}, Haifeng and
         {Dai}, Songtai and {Jiang}, Wenbin and {Liu}, Jing and {Lyu}, Yajuan and
         {Zhu}, Yong and {Wu}, Hua},
        title = "{CoKE: Contextualized Knowledge Graph Embedding}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
         year = "2019",
        month = "Nov",
          eid = {arXiv:1911.02168},
        pages = {arXiv:1911.02168},
archivePrefix = {arXiv},
       eprint = {1911.02168},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191102168W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{paper:2019arXiv191005069S,
       author = {{Shen}, Tao and {Geng}, Xiubo and {Qin}, Tao and {Guo}, Daya and
         {Tang}, Duyu and {Duan}, Nan and {Long}, Guodong and {Jiang}, Daxin},
        title = "{Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Oct",
          eid = {arXiv:1910.05069},
        pages = {arXiv:1910.05069},
archivePrefix = {arXiv},
       eprint = {1910.05069},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191005069S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




%@ARTICLE{article:rehurek_lrec,
%      title = "Software Framework for Topic Modelling with Large Corpora",
%      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
%      booktitle = {{Proceedings of the LREC 2010 Workshop on New
%           Challenges for NLP Frameworks}},
%      pages = {45--50},
%      year = 2010,
%      month = May,
%      day = 22,
%      publisher = {ELRA},
%      address = {Valletta, Malta},
%      note={\url{http://is.muni.cz/publication/884893/en}},
%      language={English}
%}
%
%@MISC{article:thought2vec-geoffrey-hinton,
% author  = {Hannah Devlin},
% date    = {2015-05-21},
% title   = {Google a step closer to developing machines with human-like intelligence},
% journal = {The Guardian},
% url     = {\url{https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence}},
% urldate = {2019-05-26}
%}
