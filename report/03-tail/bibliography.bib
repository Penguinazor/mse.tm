@article{studies:state_of_ai_2019,
abstract = {We believe that AI will be a force multiplier on technological progress in our increasingly digital, data-driven world. This is because everything around us today, ranging from culture to consumer products, is a product of intelligence. In this report, we set out to capture a snapshot of the exponential progress in AI with a focus on developments in the past 12 months. Consider this report as a compilation of the most interesting things we've seen that seeks to trigger an informed conversation about the state of AI and its implication for the future. This edition builds on the inaugural State of AI Report 2018, which can be found here. We consider the following key dimensions in our report: Research: Technology breakthroughs and their capabilities. Talent: Supply, demand and concentration of talent working in the field. Industry: Large platforms, financings and areas of application for AI-driven innovation today and tomorrow. China: With two distinct internets, we review AI in China as its own category. Politics: Public opinion of AI, economic implications and the emerging geopolitics of AI.},
author = {Benaich, Nathan and Hogarth, Ian},
file = {:https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430:pdf},
mendeley-groups = {Master},
pages = {126},
title = {{State of AI 2019}},
url = {\url{https://www.stateof.ai/}},
year = {2019}
}

@article{report:Kelnar2019,
abstract = {A list of the human tasks artificial intelligence has mastered.},
author = {Kelnar, David},
booktitle = {MMC Ventures},
file = {:Users/romain.claret/Downloads/The-State-of-AI-2019-Divergence.pdf:pdf},
mendeley-groups = {Master},
pages = {151},
title = {{The State of AI}},
url = {https://www.stateofai2019.com/summary/},
year = {2019}
}


@article{papers:gpt2,
abstract = {Natural language processing tasks, such as ques- tion answering, machine translation, reading com- prehension, and summarization, are typically approached with supervised learning on task- specific datasets. We demonstrate that language models begin to learn these tasks without any ex- plicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the an- swers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and in- creasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested lan- guage modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain co- herent paragraphs of text. These findings suggest a promising path towards building language pro- cessing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/romain.claret/Desktop/TM/Papers/Language Models are Unsupervised Multitask Learners.pdf:pdf},
mendeley-groups = {Master},
title = {{Language Models are Unsupervised Multitask Learners}},
year = {2018}
}

@article{paper:tanon2016,
abstract = {Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two ma-jor collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Pri-mary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.},
author = {Tanon, Thomas Pellissier and Vrande{\v{c}}{\'{i}}c, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia},
doi = {10.1145/2872427.2874809},
file = {:Users/romain.claret/Downloads/44818.pdf:pdf},
isbn = {9781450341431},
journal = {25th International World Wide Web Conference, WWW 2016},
keywords = {Crowdsourcing Systems,Freebase,Semantic Web,Wikidata},
mendeley-groups = {Master},
pages = {1419--1428},
title = {{From freebase to wikidata: The great migration}},
year = {2016}
}

@article{paper:bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
doi = {10.1145/1376616.1376746},
file = {:Users/romain.claret/Downloads/Freebase$\backslash$: A Collaboratively Created Graph Database For Structuring Human Knowledge.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Design,Human factors,Languages},
mendeley-groups = {Master},
pages = {1247--1249},
title = {{Freebase: A collaboratively created graph database for structuring human knowledge}},
year = {2008}
}

@inproceedings{paper:gittens-etal-2017-skip,
    title = "Skip-Gram âˆ’ {Z}ipf + Uniform = Vector Additivity",
    author = "Gittens, Alex  and
      Achlioptas, Dimitris  and
      Mahoney, Michael W.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P17-1007",
    pages = "69--76",
    abstract = "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected {``}side-effect{''} of such models is that their vectors often exhibit compositionality, i.e., \textit{adding} two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., {``}man{''} + {``}royal{''} = {``}king{''}. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",
}

@article{paper:Yin2017,
abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
archivePrefix = {arXiv},
arxivId = {1702.01923},
author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tze, Hinrich},
eprint = {1702.01923},
file = {:Users/romain.claret/Desktop/TM/Papers/Comparative Study of CNN and RNN for Natural Language Processing.pdf:pdf},
mendeley-groups = {Master},
title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
url = {http://arxiv.org/abs/1702.01923},
year = {2017}
}

@article{paper:Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:Users/romain.claret/Desktop/TM/Papers/NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
mendeley-groups = {Master},
pages = {1--15},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}

@article{paper:Tai2015,
abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.00075v3},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
eprint = {arXiv:1503.00075v3},
file = {:Users/romain.claret/Desktop/TM/Papers/Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {1556--1566},
title = {{Improved semantic representations from tree-structured long short-Term memory networks}},
volume = {1},
year = {2015}
}



@article{paper:Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v5},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v5},
file = {:Users/romain.claret/Desktop/TM/Papers/Attention Is All You Need.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Master},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-December},
year = {2017}
}


@article{paper:Lucy2017,
abstract = {Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.11168v1},
author = {Lucy, Li and Gauthier, Jon},
doi = {10.18653/v1/w17-2810},
eprint = {arXiv:1705.11168v1},
file = {:Users/romain.claret/Downloads/Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning.pdf:pdf},
mendeley-groups = {Master},
pages = {76--85},
title = {{Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning}},
year = {2017}
}


@article{paper:Dong2015,
abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
author = {Dong, Li and Wei, Furu and Zhou, Ming and Xu, Ke},
file = {:Users/romain.claret/Desktop/TM/Papers/Question Answering over Freebase with Multi-Column Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {260--269},
title = {{Question answering over freebase with multi-column convolutional neural networks}},
volume = {1},
year = {2015}
}


@article{paper:Severyn2016,
abstract = {In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.},
archivePrefix = {arXiv},
arxivId = {1604.01178},
author = {Severyn, Aliaksei and Moschitti, Alessandro},
eprint = {1604.01178},
file = {:Users/romain.claret/Desktop/TM/Papers/Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {Master},
title = {{Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.01178},
year = {2016}
}


@article{paper:Chen2015,
abstract = {{\textcopyright} 2015 Association for Computational Linguistics. Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-The-Art methods.},
author = {Chen, Yubo and Xu, Liheng and Liu, Kang and Zeng, Daojian and Zhao, Jun},
file = {:Users/romain.claret/Desktop/TM/Papers/Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Master},
pages = {167--176},
title = {{Event extraction via dynamic multi-pooling convolutional neural networks}},
volume = {1},
year = {2015}
}


@article{paper:turing,
  added-at = {2012-06-19T15:53:23.000+0200},
  author = {Turing, A. M.},
  biburl = {https://www.bibsonomy.org/bibtex/2c6b8db241dec2cec3477ce771abebb8f/jaeschke},
  copyright = {Copyright © 1950 Oxford University Press},
  interhash = {3f7a151a4f79fe75b4bb148b41279a9b},
  intrahash = {c6b8db241dec2cec3477ce771abebb8f},
  issn = {00264423},
  journal = {Mind},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Oct., 1950},
  keywords = {},
  language = {English},
  number = 236,
  pages = {433--460},
  publisher = {Oxford University Press on behalf of the Mind Association},
  series = {New Series},
  timestamp = {2012-06-19T15:53:23.000+0200},
  title = {Computing Machinery and Intelligence},
  url = {http://www.jstor.org/stable/2251299},
  volume = 59,
  year = 1950
}

@inproceedings{paper:devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{paper:Karras2019stylegan2,
  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  journal = {CoRR},
  volume  = {abs/1912.04958},
  year    = {2019},
}

@inproceedings{paper:rajpurkar-etal-2018-know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@ARTICLE{paper:word2vec,
       author = {{Mikolov}, Tomas and {Chen}, Kai and {Corrado}, Greg and {Dean}, Jeffrey},
        title = "{Efficient Estimation of Word Representations in Vector Space}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2013",
        month = "Jan",
          eid = {arXiv:1301.3781},
        pages = {arXiv:1301.3781},
archivePrefix = {arXiv},
       eprint = {1301.3781},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1301.3781M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{paper:glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = "GloVe: Global Vectors for Word Representation",
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}





@misc{blog:intro_knowledge_graph,
author = {Singhal, Amit},
title = {Official Google Blog: Introducing the Knowledge Graph: things, not strings},
howpublished = {\url{https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html}},
month = {May},
year = {2012},
note = {(Accessed on 10/09/2019)}
}

@misc{website:wikidata,
author = {Foundation, Wikimedia},
title = {Wikidata},
howpublished = {\url{https://www.wikidata.org/wiki/Wikidata:Main_Page}},
month = {October},
year = {2012},
note = {(Accessed on 10/09/2019)}
}

@misc{online:gartner_2019_ai_survey,
author = {Rowsell-Jones, Andy and Howard, Chris},
title = {2019 CIO Survey: CIOs Have Awoken to the Importance of AI},
howpublished = {\url{https://www.gartner.com/en/documents/3897266/2019-cio-survey-cios-have-awoken-to-the-importance-of-ai}},
month = {January},
year = {2019},
note = {(Accessed on 10/09/2019)}
}

@misc{online:futurism_history_infography,
author = {Futurism, LLC},
title = {The History of Chatbots Infographic},
howpublished = {\url{https://futurism.com/images/the-history-of-chatbots-infographic}},
month = {January},
year = {2016},
note = {(Accessed on 10/09/2019)}
}

@misc{website:eliza,
author = {Michal Wallace \& George Dunlop},
title = {Eliza, the Rogerian Therapist},
howpublished = {\url{http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm}},
month = {January},
year = {1999},
note = {(Accessed on 10/09/2019)}
}

@misc{website:person_does_not_exist,
author = {Phil Wang},
title = {This Person Does Not Exist},
howpublished = {\url{https://www.thispersondoesnotexist.com}},
year = {2019},
note = {(Accessed on 10/09/2019)}
}

@misc{wikipedia:snn,
  author = "{Glosser.ca}",
  title = "File:Colored neural network.svg",
  year = "2013",
  url = "https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg",
  note = "[Online; accessed 26-May-2019]"
  license = "Public domain"
}


%@ARTICLE{article:rehurek_lrec,
%      title = "Software Framework for Topic Modelling with Large Corpora",
%      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
%      booktitle = {{Proceedings of the LREC 2010 Workshop on New
%           Challenges for NLP Frameworks}},
%      pages = {45--50},
%      year = 2010,
%      month = May,
%      day = 22,
%      publisher = {ELRA},
%      address = {Valletta, Malta},
%      note={\url{http://is.muni.cz/publication/884893/en}},
%      language={English}
%}
%
%@MISC{article:thought2vec-geoffrey-hinton,
% author  = {Hannah Devlin},
% date    = {2015-05-21},
% title   = {Google a step closer to developing machines with human-like intelligence},
% journal = {The Guardian},
% url     = {\url{https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence}},
% urldate = {2019-05-26}
%}
