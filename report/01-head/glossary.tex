% ---------------------------------------------------------------------
% Cloned from the HES-SO//Master canvas 2019
% ---------------------------------------------------------------------
%\acrshort{mru}
%\gls{tic}

% ---------------------------------------------------------------------
% GLOSSARY
% format:  \newglossaryentry{<label>}{<settings>}
% example: \newglossaryentry{adagrad}
% ---------------------------------------------------------------------
\newglossaryentry{model}
{
  name={Model},
  description={In \glsfirst{ml}, a Model is the representation of the assumptions made by the algorithm during the training phase. Models are used to output a result based on a provided input and the learned patterns}
}

\newglossaryentry{transformer}
{
  name={Transformer},
  description={In \glsfirst{nlp}, Transformers are similar to \glsfirst{seq2seq} architectures but use a parallelized \glsfirst{attention}}
}

\newglossaryentry{gseq2seq}
{
  name={Sequence-to-Sequence},
  description={In \glsfirst{ml}, a Sequence-to-Sequence or Seq2Seq is an \glsfirst{enc-dec} Neural Network that, for a given sequence of elements as input, outputs another sequence of elements}
}
\newglossaryentry{seq2seq}
{
  type=\acronymtype,
  name={Seq2Seq},
  description={Sequence-to-Sequence},
  first={Sequence-to-Sequence (Seq2Seq)\glsadd{gseq2seq}},
  see=[Glossary:]{gseq2seq}
}

\newglossaryentry{enc-dec}
{
  name={Encoder-Decoder},
  description={In \glsfirst{ml}, Encoder-Decoder are two \glspl{nn} that work in pair. The Encoder generates a fixed-size output vector from any sized input vector, and the Decoder generates from the Encoder's output a vector that could be of any size}
}

\newglossaryentry{attention}
{
  name={Attention Mechanism},
  description={In \glsfirst{nlp}, the Attention Mechanism is an algorithm used to compute the relational weight between elements in a sequence of elements e.g., words}
}

\newglossaryentry{model-ft}
{
  name={Model Fine-Tuning},
  description={In \glsfirst{ml}, Fine-Tuning a \glsfirst{model} is the technique of using a trained \glsfirst{nn} model as a base and tune it for a specific task}
}

\newglossaryentry{gkg}
{
  name={Knowledge Graph},
  description={In \glsfirst{is}, a Knowledge Graph is a \glsfirst{kb} organized as a graph using semantics}
}
\newglossaryentry{kg}
{
  type=\acronymtype,
  name={KG},
  description={Knowledge Graph},
  first={Knowledge Graph (KG)\glsadd{gkg}},
  see=[Glossary:]{gkg}
}

\newglossaryentry{gkb}
{
  name={Knowledge Base},
  description={In \glsfirst{is}, a Knowledge Base is a Knowledge Representation using a \glsfirst{linked-data} database for storing and interlinking structured and unstructured data using a standard}
}
\newglossaryentry{kb}
{
  type=\acronymtype,
  name={KB},
  description={Knowledge Base},
  first={Knowledge Base (KB)\glsadd{gkb}},
  see=[Glossary:]{gkb}
}


\newglossaryentry{linked-data}
{
  name={Linked Data},
  description={In \glsfirst{is}, Linked Data is a structured interlinked database mainly used for semantic queries}
}

\newglossaryentry{glm}
{
  name={Language Model},
  plural={Language Models},
  description={In \glsfirst{nlp}, a Language Model is a \glsfirst{model} trained to provide likelihood probabilities of a following sequence of words in addition to providing the probability for each sequences of words}
}
\newglossaryentry{lm}
{
  type=\acronymtype,
  name={LM},
  description={Language Model},
  first={Language Model (LM)\glsadd{glm}},
  plural={Language Models (LMs)\glsadd{glm}},
  see=[Glossary:]{glm}
}

\newglossaryentry{gmrr}
{
  name={Mean Reciprocal Rank},
  description={In \glsfirst{ir}, the Mean Reciprocal Rank provides a statistic measure of the quality of a returned ranked list of items for a query. MRR takes into account only the highest-ranked relevant item to the query}
}
\newglossaryentry{mrr}
{
  type=\acronymtype,
  name={MRR},
  description={Mean Reciprocal Rank},
  first={Mean Reciprocal Rank (MRR)\glsadd{gmrr}},
  see=[Glossary:]{gmrr}
}


\newglossaryentry{sh}
{
  name={Single-Hop},
  description={In \glsfirst{qa} Systems, a Single-Hop implies that the answer is within a single \gls{hop} of the question. Generally, a unique predicate separates the question subject and the answer object}
}

\newglossaryentry{hop}
{
  name={Hop},
  description={In \glsfirst{qa} Systems, a Hop is a quantitative measure of the number of supporting facts or combinations necessary between indirectly related pieces of information to provide an answer}
}


\newglossaryentry{mh}
{
  name={Multi-Hop},
  description={In \glsfirst{qa} Systems, a Multi-Hop implies that the answer is within multiple \gls{hop} of the question. In other words, the answer requires a combination of different pieces of information to be answerable. Generally, extra qualifying Subject-Predicate-Object Tuples (SPOs) are separating the question Subject and the answer Object}
}

\newglossaryentry{we}
{
  name={Word Embedding},
  description={In \glsfirst{nlp}, the Word Embedding is a technique for word representation as vectors in an embedding matrix. Additionally, Word Embedding has often the particularity of preserving the semantical analogies of word-vectors}
}

\newglossaryentry{pos}
{
  name={Part-of-Speech},
  description={In \glsfirst{nlp}, Part-of-Speech is a technique used to categorize words that behave syntactically similarly}
}

\newglossaryentry{pos-tag}
{
  name={Part-of-Speech Tagging},
  description={In \glsfirst{nlp}, The Part-of-Speech Tagging is extending the \glsfirst{pos} by adding a label to the word depending on its context (the neighboring words)}
}


\newglossaryentry{ner}
{
  name={Named-Entity Recognition},
  description={In \glsfirst{ie}, Named-Entity Recognition is a technique used to extract, from unstructured texts, words from a predefined vocabulary}
}

\newglossaryentry{nel}
{
  name={Named-Entity Linking},
  description={In \glsfirst{nlp}, Named-Entity Linking extends the \glsfirst{ner} by providing a unique identifier to each word allowing a mapping in various databases (useful in translations)}
}

\newglossaryentry{open-domain}
{
  name={Open Domain},
  description={In \glsfirst{ir}, the support of Open Domain questions provides no restriction to the subject of the questions}
}

\newglossaryentry{closed-ended}
{
  name={Close-ended},
  description={A closed-ended question is designed to allow a limited amount of responses}
}

\newglossaryentry{zero-shot}
{
  name={Zero-Shot Learning},
  description={In \glsfirst{ml}, Zero-Shot Learning is a technique used to solve tasks without training on examples}
}

\newglossaryentry{few-shot}
{
  name={Few-Shot Learning},
  description={In \glsfirst{ml}, Few-Shot Learning is a technique used to solve tasks with a very small amount of training data}
}

\newglossaryentry{generative-model}
{
  name={Generative Model},
  plural={Generative Models},
  description={In \glsfirst{ml}, Generative Models generate random outputs from a single input by using the probability of observing the output based on the input. In other words, it models the probability of observation for a given target}
}

\newglossaryentry{generative}
{
  name={Generative},
  description={In the context of the thesis, we are using the generic word \say{Generative} as the concept of an algorithm able to generate outputs in a meaningful but unpredictable manner from an input, which includes \glspl{lm} and \glspl{generative-model}}
}

\newglossaryentry{gweak-ai}
{
  name={Weak Artificial Intelligences},
  description={In the context of the \glsfirst{ai}, }
}

\newglossaryentry{weak-ai}
{
  type=\acronymtype,
  name={Weak AI},
  description={Weak Artificial Intelligence},
  first={Weak Artificial Intelligence (Weak AI)\glsadd{gweak-ai}},
  see=[Glossary:]{gweak-ai}
}

\newglossaryentry{gl}
{
  name={Ground Learning},
  description={In the context of the \glsfirst{ai}, Grounded Learning is based on the Grounded theory from the social sciences, which uses inductive reasoning. The mechanism combines structured and unstructured data as small conceptual parts to then apply machine reasoning inductively}
}


\newglossaryentry{gmdp}
{
  name={Markov Decision Process},
  description={In the context of the \glsfirst{rl}, this process models the ability to predict the next state of a finite-state-machine-like process, such as a game, using only the information contained in the present state}
}

\newglossaryentry{mdp}
{
  type=\acronymtype,
  name={MDP},
  description={Markov Decision Process},
  first={Markov Decision Process (MDP)\glsadd{gmdp}},
  see=[Glossary:]{gmdp}
}

\newglossaryentry{grl}
{
  name={Reinforcement Learning},
  description={In \glsfirst{ml}, Reinforcement Learning combines a \glsfirst{mdp} environment with an approach similar to \glsfirst{ul} as this type of learning does not require labelled data. The particularity of this technique is that it uses a notion of rewards to predict the best next step by running a large amount of simulations as training}
}

\newglossaryentry{rl}
{
  type=\acronymtype,
  name={RL},
  description={Reinforcement Learning},
  first={Reinforcement Learning (RL)\glsadd{grl}},
  see=[Glossary:]{grl}
}

\newglossaryentry{gul}
{
  name={Unsupervised Learning},
  description={In \glsfirst{ml}, Unsupervised Learning implies the use of unlabelled datasets to perform the training}
}

\newglossaryentry{ul}
{
  type=\acronymtype,
  name={UL},
  description={Unsupervised Learning},
  first={Unsupervised Learning (UL)\glsadd{gul}},
  see=[Glossary:]{gul}
}

\newglossaryentry{gsl}
{
  name={Supervised Learning},
  description={In \glsfirst{ml}, this type of learning implies the uses of labelled datasets to perform the training}
}

\newglossaryentry{sl}
{
  type=\acronymtype,
  name={SL},
  description={Supervised Learning},
  first={Supervised Learning (SL)\glsadd{gsl}},
  see=[Glossary:]{gsl}
}

\newglossaryentry{gal}
{
  name={Adversarial Learning},
  description={In \glsfirst{ml}, the concept of this technique relies on trying to fool models via malicious inputs. Interpretable as a game, a model plays with itself and modifies the input in such a way that the model will recognize the input as a different one, and then learn from its mistake}
}

\newglossaryentry{al}
{
  type=\acronymtype,
  name={AL},
  description={Adversarial Learning},
  first={Adversarial Learning (AL)\glsadd{gal}},
  see=[Glossary:]{gal}
}

\newglossaryentry{gmu}
{
  name={Machine Understanding},
  description={In \glsfirst{ml}, we use Machine Understanding as the ability to represent knowledge as atomic building blocks and fundamental relations}
}

\newglossaryentry{mu}
{
  type=\acronymtype,
  name={MU},
  description={Machine Understanding},
  first={Machine Understanding (MU)\glsadd{gmu}},
  see=[Glossary:]{gmu}
}

\newglossaryentry{gmr}
{
  name={Machine Reasoning},
  description={In \glsfirst{ml}, Machine Reasoning represent the ability to apply reasoning for a given input by using knowledge representations and logic patterns such as inductions, analogies, or abductions}
}

\newglossaryentry{mr}
{
  type=\acronymtype,
  name={MR},
  description={Machine Reasonsing},
  first={Machine Reasoning (MR)\glsadd{gmr}},
  see=[Glossary:]{gmr}
}


\newglossaryentry{gsnn}
{
  name={Shallow Neural Network},
  description={In \glsfirst{ml}, similar to \glsfirst{dl}, Shallow Neural Networks have a \glsfirst{enc-dec} approach by having a single hidden layer, which often has a large amount of parameters}
}

\newglossaryentry{snn}
{
  type=\acronymtype,
  name={SNN},
  description={Shallow Neural Network},
  first={Shallow Neural Network (SNN)\glsadd{gsnn}},
  see=[Glossary:]{gsnn}
}


\newglossaryentry{gbilm}
{
  name={Bidirectional Language Model},
  description={In \glsfirst{nlp}, a Bidirectional Language Model represents a \glsfirst{lm} combining the forward pass and the backward pass of the same corpora}
}

\newglossaryentry{bilm}
{
  type=\acronymtype,
  name={biLM},
  description={Bidirectional Language Model},
  first={Bidirectional Language Model (biLM)\glsadd{gbilm}},
  see=[Glossary:]{gbilm}
}

\newglossaryentry{gbert}
{
  name={Bidirectional Encoder Representations from Transformers},
  description={In \glsfirst{nlp}, \textit{Google} BERT is a large \gls{transformer}-based model trained at predicting masked tokens within sequences}
}

\newglossaryentry{bert}
{
  type=\acronymtype,
  name={BERT},
  description={Bidirectional Encoder Representations from Transformers},
  first={Bidirectional Encoder Representations from Transformers (BERT)\glsadd{gbert}},
  see=[Glossary:]{gbert}
}


\newglossaryentry{ggpt2}
{
  name={Generative Pre-Training 2},
  description={In \glsfirst{nlp}, \textit{Open-AI} GPT-2 is a large \gls{generative-model} using \glspl{transformer} to generate outputs based on the probability of the token observation}
}

\newglossaryentry{gpt2}
{
  type=\acronymtype,
  name={GPT-2},
  description={Generative Pre-Training 2},
  first={Generative Pre-Training 2 (GPT-2)\glsadd{ggpt2}},
  see=[Glossary:]{ggpt2}
}


\newglossaryentry{bleu}
{
  name={BLEU},
  description={In \glsfirst{nlp}, The Bilingual Evaluation Understudy (BLEU) is an evaluation metrics particularly popular in machine translation as the processing is automatic}
}

\newglossaryentry{oracle}
{
  name={Oracle},
  description={In \glsfirst{ml}, an Oracle is defined as an entity that knows the ground truth to all questions. An Oracle can be a human, or an algorithm querying a database with no errors}
}

\newglossaryentry{wikidata}
{
  name={Wikidata},
  description={Wikidata is a community-based \gls{kb}, based on Freebase originally. It stores its data into a linked-data format with Subject-Predicate-Object Tuples (SPOs)}
}

\newglossaryentry{f1}
{
  name={F1},
  description={In Statistics, the F1 score is used to compute an accuracy metric, using Precision and Recall. \[ 2*((P*R)/(P+R)) \] }
}


%\gls{tf-idf}: Used to set the word importance in corpora.
%\gls{cbow} [\ref{analyse:cbow}]: Counts the words occurrences throughout in corpus.
%Skip-Grams [\ref{analyse:skip-grams}]: Counts the occurrences of the character throughout in corpus.
%Topic modeling: Text clustering providing meaningful information to discover hidden structures via text chunking to identify the parts of the sentence in relation to each other.
%Segmentation: Split corpus into predefined parts, such as: \textit{sentences, paragraphs, chapters, etc.}
%Tokenization: Split the sentences into words.
%Tagging: Based on a pre-made dictionary, it gives a new layer of meaning to the word, such as: \textit{verb, adverb, noun, people name, locations, number, etc.}
%Dictionary: Use of tokenized words to build a dictionary, which could contain the word occurrences.
% Stop Words: Ignoring words only used as liaisons, and not containing information, such as: \textit{and, or, etc.}
% Stemming: Uniformizing words to their root by removing the prefix and suffix, such as: \textit{remake and loveable}.
% Lemmatization: Replace the words to their base form, such as \textit{conjugated verb}.


% ---------------------------------------------------------------------
% Acronymes
% format:  \newacronym{<label>}{<abbrv>}{<full>}
% example: \newacronym{13c}{13C}{carbon-13}
% ---------------------------------------------------------------------
\newacronym{mru}{MRU}{Master Research Units}
\newacronym{ict}{ICT}{Information and Communications Technologies}
\newacronym{ann}{ANN}{Artificial Neural Networks}
\newacronym{dnn}{DNN}{Deep Neural Networks}
\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{nlu}{NLU}{Natural Language Understanding}
\newacronym{nlg}{NLG}{Natural Language Generation}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{aiml}{AIML}{Artificial Intelligence Markup Language}
\newacronym{ai}{AI}{Artificial Intelligence}
\newacronym{agi}{AGI}{Artificial General Intelligence}
\newacronym{ani}{ANI}{Artificial Narrow Intelligence}
\newacronym{asi}{ASI}{Artificial Super Intelligence}
\newacronym{scifi}{Sci-Fi}{Science Fiction}
\newacronym{faq}{FAQ}{Frequently Asked Questions}
\newacronym{rnn}{RNN}{Recurrent Neural Network}
\newacronym{ir}{IR}{Information Retrieval}
\newacronym{is}{IS}{Information Systems}
\newacronym{dm}{DM}{Data Mining}
\newacronym{bd}{BD}{Big Data}
\newacronym{dl}{DL}{Deep Learning}
\newacronym{nn}{NN}{Neural Network}
\newacronym{nl}{NL}{Natural Language}
\newacronym{tf-idf}{TF-IDF}{Term Frequency-Inverse Document Frequency}
\newacronym{cbow}{CBOW}{Continuous Bag of words}
\newacronym{aws}{AWS}{Amazon Web Services}
\newacronym{dp}{DP}{Deepening Project}
\newacronym{mt}{MT}{Master's Thesis}
\newacronym{poc}{POC}{Proof of Concept}
\newacronym{mvp}{MVP}{Minimum Viable Product}
\newacronym{kiss}{KISS}{Keep It Stupid Simple}
\newacronym{qa}{QA}{Question-Answering}
\newacronym{gs}{GS}{Generative System}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{mn}{MN}{Memory Network}
\newacronym{spo}{SPO}{Subject-Predicate-Object Tuple}
\newacronym{ie}{IE}{Information Extraction}
\newacronym{sota}{SOTA}{State of the Art}
\newacronym{gan}{GAN}{Generative Adversarial Networks}
\newacronym{oov}{OOV}{Out-of-Vocabulary}
\newacronym{ce}{CE}{Character Embedding}
\newacronym{cwe}{CWE}{Context-based Word Embedding}
\newacronym{squad}{SQuAD}{Stanford Question Answering Dataset}
\newacronym{coqa}{CoQa}{Conversational Question Answering}
\newacronym{quac}{QuAC}{Question Answering in Context}
\newacronym{at}{AT}{Adversarial Training}
\newacronym{kd}{KD}{Knowledge Distillation}
\newacronym{lstm}{LSTM}{Long Short-Term Memory}
\newacronym{glue}{GLUE}{General Language Understanding Evaluation}

