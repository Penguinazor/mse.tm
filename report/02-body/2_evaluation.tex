\chapter{Evaluation}
\label{chap:evaluation}
%performances

In this chapter, we overview current evaluations in \gls{nlp} for the two tasks our project is combining, \gls{qa} systems and computer-generated dialogues. Often, determining if a model is working as expected is a hard task, it depends on the tasks itself the datasets used. Naively, one could build a complex supervised protocol to evaluate a model's success. Still, often it is not enough due to the chaotic nature of training or to multiple exceptions to handle; indeed, it would require an unrealistic amount of human-power to build a general evaluation protocol, particularly for \gls{nl} tasks. Instead, it is common to make and combine grounded tasks evaluation and sub-tasks to get metrics.


\section{Question Answering Systems}
Empirically to the \gls{qa} Datasets \gls{sota} from chapter \ref{dataset:qa}, we targeted our research at providing comparative results to our later described \gls{qa} system GraphQA (see Chapter \ref{chap:graphqa}). We explored additionally results from \gls{squad} (see Chapter \ref{dataset:squad}) and (see Chapter \gls{coqa}) as grope to \gls{transformer} performances at \gls{qa} tasks. With difficulty, we represented in the Table \ref{tab:qa_overview_benchmarking} benchmark results; indeed, it appeared that the majority of baseline competitor to the \gls{qa} task are using different datasets, making particularly difficult to syntethis as no evaluation dataset baseline is defined yet.

\subsection{CONVEX}
\label{eval:convex}
CONVEX \autocite{paper:convex} has been developed to handle the ConvQuestions dataset (see Chapter \ref{dataset:convquestions}), and at time of writing, it was not evaluated on additional datasets. CONVEX uses subgraphs to manipulate Wikidata entities extracted via TAGME \autocite{paper:CIKM-2010-FerraginaS} from the Wikidata \gls{kb} to answer turn-based \gls{mh} conversational questions. The authors use the \gls{mrr} metric in addition to the Top-1 and Top-5.

\subsection{qAnswer}
Initially build for the DBpedia \gls{kb}, qAnswer converts \gls{nl} questions into SPARQL query with a template-based model (see Chapter \ref{chatbot:templates}), and later got extended to the Wikidata \gls{kb}. It uses Wikipedia pages to extract lexicalizations and match \gls{nl}, entities, and relations. The authors originally used F1 as metric on the QALD-5 test corpus \autocite{paper:Lopezetal2013}, and later got evaluated on the ConvQuestions dataset (see Chapter \ref{dataset:convquestions}) by the CONVEX authors (see Chapter \ref{eval:convex}) with \gls{mrr}, Top-1 and Top-5 metrics.

\subsection{Platypus}
Similar to qAnswer, Platypus \autocite{paper:InProceedingsPellissier-Tanon.P-TD-d-ACM-S_18} is a template-based model (see Chapter \ref{chatbot:templates}) trained to build SPARQL request for Wikidata as well, but has been released in open-source. It was initially trained, tested, and evaluated with F1 on the Wikidata mapped SimpleQuestions dataset (see Chapter \ref{dataset:simplequestions}). Later Playtypus got additionally evaluated on the ConvQuestions dataset (see Chapter \ref{dataset:convquestions}) by the CONVEX authors (see Chapter \ref{eval:convex}) with \gls{mrr}, Top-1 and Top-5 metrics.

\subsection{Honorable Mention}
It is no spoiler to mention that Fine-Tuned Pre-Trained Language Models \ref{nlp:transformers} are currently under the spotlights. By curiosity, we wanted to get hold of the phenomenon by investigating and reporting a few comparative results of \gls{transformer}, \gls{lstm}, and \gls{rnn} Models. We noticed based on the Table \ref{tab:qa_overview_benchmarking} that \gls{bert}-based \autocite{paper:devlin-etal-2019-bert} models in their base form with pre-training are largely outperforming non-\gls{transformer}-based models. As a final note, we didn't judge it necessary to include extended \gls{bert}-based models to the table as we believe that it is a field of study by itself and out of our scope. However, we just wanted to mention that, at time of writing, the best baseline in \gls{coqa} (see Chapter \ref{dataset:coqa}) leaderboard with an overall 90.7\%, is a compositional model combining RoBERTa \autocite{paper:journals/corr/abs-1907-11692} (BERT-based), \gls{at}, and \gls{kd} \autocite{paper:2019arXiv190910772J}, and as a friendly reminder, that Human Performance is set at 88.8\% on the same dataset.


\section{Generative Systems}
As mentioned in chapter \ref{dataset:dialogue}, it appeared that so far, progress is in standby for the computer-generated text \gls{nlp} task, taking machine translation and the \gls{oracle} approach apart. As far as our research went, we found two papers supporting interesting facts about computer-generated texts. According to the first study \autocite{paper:journals/corr/abs-1906-04043}, the authors describe a technique to detect computed generated text by focusing on the induced artifact from generating text. The second paper \autocite{paper:10.1177/1464884916641269} performed a study on the Readersâ€™ perception of computer-generated news. They concluded that people enjoy reading computer-generated texts because it is, in fact, computer-generated, which is currently fascinating to humans; however, a long term study still has to be conducted.

\paragraph{BLEU}
\label{eval:bleu}
Originally the Bilingual Evaluation Understudy (\gls{bleu}) was created as a completely automated metric for machine translation, but is in theory, adaptable to other \gls{nlp} tasks such as \gls{nl} generation. By simply comparing the machine output and the ground truth, \gls{bleu} is relatively faster than human translators, computationally friendly, and benchmarkable. It implies that by design, \gls{bleu} does not take into account the meaning of the sentence, nor it is taking into account the sentence structure, nor it evaluates how a human would interpret the sentences. Meaning that rich languages do not evaluate well and imply that \gls{bleu} perform well in machine translation to measure entire corpora for a good reason, but it is not acceptable in our study as we expect a meaningful human-like evaluation.

\paragraph{ROUGE}
ROUGE is a \gls{bleu} (see chapter \ref{eval:bleu}) adaptation focusing on Recall instead of Precision, by evaluating the ground truth to the output.

\paragraph{GLUE}
Since 2019, the \glsfirst{glue} benchmark is a complete framework used to train, evaluate on a collection of datasets and then compare models relative to one another on various \gls{nlp} tasks. The dataset collection is composed of nine (four are kept private) relatively difficult datasets designed to test \gls{mu}, which is particularly interesting for fine-tuned models.

\paragraph{Natural Questions Corpus Metric}
As mentioned in chapter \ref{dataset:googlenatural}, \textit{Google's} Natural Questions Corpus dataset \autocite{paper:google-natural-questions} is a benchmarking approach that defined a new metric to answer evaluation combining F1 from Long Answers and Short Answers with multiple annotators, making it 25-way Annotated. The technique consists of asking annotators for given questions to say if the question is fact-seeking or not, return a long and short answer pair. As the next step, the previously annotated questions are sent to 4 annotators; whose goal is to evaluate the annotations. Based on majority expert judgements, the annotated questions are categories as \say{Correct}, \say{Debatable} or \say{Wrong}. Finally, the annotated questions evaluation is measured by calculating the Precision and Recall of the long and short answers.

\paragraph{Humans}
Although it is evident that automatic evaluation metrics are not entirely reliable for text related \gls{nlp} tasks. The only available solution so far is the use of Humans to perform manual validation, either by Crowdsourcing via Mechanical Trucks, or by asking colleagues. However, often the results even obtained from humans are not qualitatively optimal. Indeed,  distraction is human nature, particularly in Mechanical Trucks setups, resulting in protocols such as \textit{Google's} Natural Questions Corpus dataset (see chapter \ref{dataset:googlenatural}) are established, implying multiple verifications, with the goal to produced the optimal labelings and evaluations.

\setlength{\tabcolsep}{7pt}
\renewcommand{\arraystretch}{1.1}
\begin{landscape}
\begin{longtable}[c]{@{}lcclccccc@{}}
\toprule
Models                                                         & \begin{tabular}[c]{@{}c@{}}Release \\ Date\end{tabular} & \begin{tabular}[c]{@{}c@{}}Handles \\ Nested \\ Questions\end{tabular} & \multicolumn{1}{c}{ML}                                          & \textbf{\begin{tabular}[c]{@{}c@{}}SimpleQuestions \\ F1\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}ConvQuestions \\ MRR\end{tabular}} & \begin{tabular}[c]{@{}c@{}}SQuAD2.0 \\ F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}SQuAD1.1 \\ F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}CoQa \\ F1\end{tabular} \\* \midrule
\endhead
%
\bottomrule
\endfoot
%
\endlastfoot
%
ALBERT                                                         & 2019                                                    & \textbf{Yes}                                                           & Transformer                                                     & -                                                                      & -                                                                     & 92.215\%                                               & -                                                      & -                                                  \\
\begin{tabular}[c]{@{}l@{}}BERT-base \\ finetuned\end{tabular} & 2019                                                    & \textbf{Yes}                                                           & Transformer                                                     & -                                                                      & -                                                                     & 83.061\%                                               & 93.16\%                                                & 81.1\%                                             \\
CONVEX                                                         & 2019                                                    & \textbf{Yes}                                                           & \begin{tabular}[c]{@{}l@{}}Information\\ Retrieval\end{tabular} & -                                                                      & 0.2012                                                                 & -                                                      & -                                                      & -                                                  \\
qAnswer                                                        & 2019                                                    & No                                                                     & Template                                                        & -                                                                      & 0.0294                                                                & -                                                      & -                                                      & -                                                  \\
BiDAF++                                                        & 2018                                                    & \textbf{Yes}                                                           & CNN                                                             & -                                                                      & -                                                                     & -                                                      & -                                                      & 67.8\%                                             \\
Platypus                                                       & 2018                                                    & No                                                                     & Template                                                        & 79.96\%                                                                & 0.0022                                                                & -                                                      & -                                                      & -                                                  \\
QANet                                                          & 2018                                                    & No                                                                     & Transformer                                                     & -                                                                      & -                                                                     & -                                                      & 82.7\%                                                 & -                                                  \\
DrQA                                                           & 2017                                                    & \textbf{Yes}                                                           & RNN                                                             & -                                                                      & -                                                                     & -                                                      & 79.353\%                                               & -                                                  \\
BiDaF                                                          & 2016                                                    & No                                                                     & LSTM                                                            & -                                                                      & -                                                                     & -                                                      & 81.525\%                                               & -                                                  \\
MemNet                                                         & 2015                                                    & No                                                                     & \begin{tabular}[c]{@{}l@{}}Memory \\ Network\end{tabular}       & 77.97\%                                                                & -                                                                     & -                                                      & -                                                      & -                                                  \\* \bottomrule
\caption{Question Answering Benchmarking Overview}
\label{tab:qa_overview_benchmarking}\\
\end{longtable}
\end{landscape}

