\chapter{Evaluation}
\label{chap:evaluation}

\section{Generative}
\todo{Not a lot of work has been done in the domain of computer generated text, taking appart machine translation. So far I found a paper (GLTR: Statistical Detection and Visualization of Generated Text) that talks about a technic to detect generated content. And I found an article about the Readersâ€™ perception of computer-generated news(: Credibility, expertise, and readability) which says that people like to read computer generated content because it's what it is, and humans are amazed about it, the long term have to be explored.}
\paragraph{BLEU}
\todo{The Bilingual evaluation understudy was originally made to measure machine translation and now is used for Natural language generation. The goal is to compare the machine generated output text string to its expected output, the algorithm is fast and easily computable compared to human translators. Plus, it's benchmarkable. However, it's not made to take the meaning of the sentence into account (attention), it's not taking into account the sentence structure, it's doesn't work with rich languages and it doesn't take into account how humans are actually interpreting the sentences. Basically to used only in the machine translation context to evaluate an entire corpus.}
\paragraph{ROUGE}
\todo{Adaptation from BLUE to focus on the recall instead of Precision. It checks the the reference translation to the output.}

performances

\section{QA Systems}
\section{Generative Systems}
\section{Conversational Agents}