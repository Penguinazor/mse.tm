\chapter{Discussion}
\label{chap:discussion}

The following chapter summarizes our state of mind about the project. As we choose a storytelling approach for our Master's Thesis, discussions were diluted within all chapters making the traditional academic discussion chapter, the only place to discuss our humble opinions. First of all, we want to remind that GraphQA is a \gls{poc}, and its performances are not particularly important, as it is the project itself, with its original approaches that must be evaluated. As an observation, we are using \gls{sota} technologies mostly released in late 2019 (and even for some, such as CONVEX was released during our last \gls{sota} sprint), reinforcing our statement that \gls{nlp} is driving a lot of interest lately. To structure this chapter, we will mostly talk about our main competitor and its dataset. Indeed, we indirectly spent the Master's Thesis analyzing their work, as GraphQA is using a similar sub-knowledge graph approach. We believed that it is important to discuss how our competitors perform. We will finally overview what we learned from our overall research and threw a few questions that are left to answer.


\section{ConvQuestions Dataset}
As we praised this dataset the whole thesis, we believe for good reason, as it a \gls{sota} in the field of  \gls{mh} and multi-turns. However, in this section we will overview the dataset their contribution to \gls{nlp}, the limitation, and the required improvement for future \gls{mh} and multi-turns conversational datasets.

\subsection{Data Augmentation}
As a contribution, ConvQuestion, they suggested a new data augmentation approach for \gls{nlp}. Data augmentation is very popular in the field of Computer Vision; however, it is often a bad solution in \gls{nlp} as it often uses computer-generated paraphrasing by applying \gls{we} similarities. With ConvQuestion, the authors took a clever shift by asking humans to generate the paraphrasing, with the constraint that the paraphrased sentence must be semantically equivalent and interchangeable with its original sentence. Besides, ConvQuestions guarantees that the conversations are not permuted to not alter nested questions. It won't surprise us that \gls{qa} evaluation-based datasets such as \gls{squad} will use this method to enhance their datasets. On a final note, ConvQuestions is a relatively small dataset with its 11'200 questions compared to the 150'000 questions from \gls{squad} 2.0; however, the plot twist is that ConvQuestion used a total of 1'750 unique questions to generate their dataset with only a single permutation per questions.

\subsubsection{Paraphrasing examples}
\textbf{Who is the author of the Harry Potter series?} Who wrote Harry Potter?\\
\textbf{What was the year of publication for the first book?} When was it first published?\\
\textbf{Title of the first book?} The first book was called what?\\
\textbf{What country was the book set in?} It was set in what country?\\
\textbf{Which book has the highest page count?} What's the longest book?

\subsection{Human Errors}
\label{discussion:humans}
The main limitation we noticed is the crowdsourcing itself. Even if this solution is at the time of writing, probably the best to generate \gls{nlp} datasets, Mechanical Truckers often make mistakes. Indeed, it is difficult for large datasets to have no mistakes; it is human nature to get distracted, particularly for repetitive tasks such as being a Mechanical Trucker. A common inattentiveness we noticed, in addition to laziness while paraphrasing (see the previous subsection), is that the truckers do answering mistakes and are not always respecting the format guidelines, implying 32 ($2^5$) wrong question-answers tuples for a single mistake. For a small dataset such as ConvQuestion, the ratio of unanswerable tuples can rise dramatically and induce important bias while training models. We regret that the author didn't take the time to review correctly their relatively small dataset, or implement a crowdsourced cross-checking protocol like \textit{Google} did for their Natural Dialogue dataset \autocite{paper:google-natural-questions}. 

\subsubsection{Mechanical Trucker error examples}

\paragraph{Answering Mistake}
In this situation, the Trucker provided the wrong answer to the question. \\
\textbf{Question:} When did the first The Fast and the Furious film come out?\\
\textbf{Expected answer:} 1955\\
\textbf{Wrong answer:} 22 June 2001


\paragraph{Inattentiveness} 
For this example we extrapolate the expected answer from the full \gls{nl} answer span \say{The first film came out 22 June 2001.}. \\
\textbf{Question:} When did the first The Fast and the Furious film come out? \\
\textbf{Expected answer:} 22 June 2001 \\
\textbf{Trucker's answer:} https://www.wikidata.org/wiki/Q155476\\

\paragraph{Not Respecting the Answer Guideline} 
In this example, the Trucker did not providing a \gls{nl} answer span. \\
\textbf{Question:} When was he born? \\
\textbf{Answer:} 1 August 1819 \\
\textbf{Answer Span:} 1 August 1819 \\


\subsection{GraphQA}
The previous sections would probably not exist if GraphQA didn't find them. Indeed, as we were monitoring GraphQA answers, we noticed that sometimes the answers were correct, but the benchmark said otherwise. To our regrets, due to time constraints, we could not go through the whole dataset; however, this observation implies that an application for GraphQA and its competitors could be to \gls{oracle} crowdsourced datasets.


\section{CONVEX}
In our opinion, CONVEX is a pioneer in the field of sub-knowledge graphs and we amire their approach as they inspired  GraphQA. They were chosen during our analysis as a starting project for our \gls{qa} system and planned to enhance their work. However, as stated in the GraphQA chapter \ref{chap:graphqa}, we took another path and, in the following sections, we want to discuss this decision and discuss their work as our main competitor.

\subsubsection{Initial Issues}
Our initial grip with CONVEX code was not the smoothest, as the authors provided their code in the state with few refactoring bugs. We contacted the authors and debugged the code together; then, we decided to fork the project and continue on our own by fixing and adapting the code to our needs as GraphQA progressed. After the project investigation, we noticed that we could not reproduce the results in the paper. We contacted the authors to notify the issues: they explained that they used an undisclosed split of the dataset to obtain their results, and confirmed that the code works as expected.

\subsubsection{Q0 Problems in more details}
Based on the previous authors' statement, we started to investigate further how CONVEX answers questions. As the paper acknowledges that Q0 is their bottleneck, we decided to explore further why it is indeed a bottleneck; we observed that often the answers are lucky guesses as it returns the first Object from a multi solutions Subject-Predicate tuples. E.g., \say{Who is the actor of this role in this movie?}: by design, the query would return the list of all actors to the requested movie, which we call hypothesis in GraphQA. CONVEX, in this particular case, will return the first element. Their \gls{ner} depends on TAGME, which is proprietary, meaning that it's up to a black box to return the \gls{wikidata} entity identified based on the context of the sentence. It is indeed hard to trust a proprietary system to return the right entities when multiple entities have the same name.

\subsubsection{GraphQA from Scratch}
Based on the previous analysis, it was clear that building an open-source module for Q0 was the priority task. We investigated in details how to integrate our module to the existing system, by going deeper into the code and understanding all functions. Aside from odd computations, we could not find the context-graph generation as described in the paper, so we asked the author about it. They explained that the part we are looking for, which we imagined to be the most exciting, were written in the paper with made-up values for motivational purposes. From this point, we lost all motivation to enhance CONVEX as its key feature was not implemented and required an architecture remodeling to get it to work. Annoyingly, the clock was ticking, and we could not step back to evaluate a new solution. Based on the CONVEX's motivational purpose feature, we built from scratch an open-source architecture handling scoring by design. (See chapter \ref{chap:graphqa})


\section{Lesson Learned}
This project gave us lessons about the academic experience and the field of \gls{nlp}.

\subsection{Only trust yourself}
\paragraph{Preprints} Some papers are good, but they are mostly bad as they are either repetitive or pointless with name-dropping.
\paragraph{Published in journal} Mostly good, they often do nested research by publishing a paper at each iteration of their project.
\paragraph{Published in conferences} Often useful, but be careful with the origin country.
\paragraph{Repetitivity} We realized that after a reasonable amount of paper read, we are not surprised anymore about the breakthroughs.
\paragraph{Never trust claims in articles} Always reproduce the results and inspect the given code and datasets.
\paragraph{Stay critic} Every day a new \gls{sota} or baseline is claimed, wait for reproduced results before excitement.

\subsection{Don't trust Mechanical Trucks}
As stated in the previous section \ref{discussion:humans}, it is essential when using Mechanical Trucks always to add a verification layer.


\section{Questions Left}
As our project end, in addition to questioning seeds left in chapters, we still have remaining questions that we wished we had time to explore. We listed them here as a starting point for GraphQA enhancement or other contributions.

\begin{itemize}
    \setlength\itemsep{0em}
    \item Would it be possible for \gls{qa} system to become sophist, and what would be the implications?
    \item Would it be possible to use GraphQA as a sophist chatbot?
    \item Would it be possible to train a model to use context graphs?
    \item Would it be possible to build a tool for users to create context graphs manually?
    \item Would it be possible to build a tool to parse articles and generate context graphs automatically? 
    \item Investigate the correlation between human thinking and reasoning with knowledge graphs representation.
    \item Compare knowledge graphs representation to \gls{ir} and access structure.
    \item Compare Wordnet and \gls{we} in the scope of GraphQA.
\end{itemize}












