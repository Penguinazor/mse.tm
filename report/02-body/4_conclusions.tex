\chapter{Conclusions}
\label{chap:final-conclusions}
This chapter concludes our Master's Thesis. During our journey, we had the chance to get a glance of the academic life by starting with an in-depth review of the current \gls{sota} for Chatbots and \gls{nlp} techniques, and finishing by a potential contribution to the \gls{nlp} field. Driven by passion, we cumulated well over 900 hours, but hope that our work as the purpose to enlarge, what we believe, a relatively unexplored field for \gls{qa} and potentially a new \gls{nlp} approach for grounded learning. We predict to see pre-trained language models and pre-trained generative language models being fine-tuned on sub-knowledge graphs shortly. 

Even if GraphQA, in its final form, is different from the expected version from the analysis, we believe that we could shift the project in a better direction as an overall contribution to \gls{nlp}. Indeed, we achieved at building a \gls{poc} of a \gls{mh} and multi-turns conversations Sub-\gls{kg} \gls{qa} Chatbots outputting \gls{nl} answer, by orchestrating multiple models with \gls{zero-shot} approach, which is not currently a trend in the field of \gls{nlp}. We hope that our contribution will serve newcomers in grounded tasks. Note that our work can be used for training models to perform similarly on subgraphs.


\section{Final words}
We observed a valuable statement with this project: simpler a concept or a model is, the best it is. Compared with nature, the simplest survives the best. In \gls{nlp} at the time of writing, \glspl{transformer} are currently leading with their relatively simple architecture; we believe that their success results from the simplicity.

We wish that additional work would be done towards a multiple-brain strategy with grounded tasks, and we believe that GraphQA could be taken as an example to break complexes tasks such as \gls{nlp} into smaller and simpler tasks to handle by models. 

A lot of adaptation is being made from a \gls{ml} field to others. However, we believe that instead of adapting technologies from other fields, the next breakthrough is the combination of various \gls{ml} fields such as Machine Vision, or Sensory Robotics. Building a Multi-Domain Grounded Task Generation model using Grounded Learning is set to become a new standard in \glsfirst{mr} and \glsfirst{mu}.

We conclude by assuming that our GraphQA approach is slightly analogically to human's knowledge representation and reasoning, as we use multiple modules to accomplish a composite result. However, we hope that more research will be done on subgraphs and perform a comparative study with the way humans are processing knowledge.
